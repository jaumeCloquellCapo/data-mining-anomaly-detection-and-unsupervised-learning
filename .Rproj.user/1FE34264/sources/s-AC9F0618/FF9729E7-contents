Skip to content
Why GitHub? 
Business
Explore 
Marketplace
Pricing 

Search

Sign in
Sign up
1 0 0 Mancuerna/DATCOM
 Code  Issues 0  Pull requests 0  Projects 0  Insights
Join GitHub today
GitHub is home to over 28 million developers working together to host and review code, manage projects, and build software together.

DATCOM/MDA/arules/arules.Rmd
d2c6eb6  on 19 Feb 2018
@Mancuerna Mancuerna Arules finished.
     
237 lines (173 sloc)  13 KB
---
title: "Reglas de asociación"
author: "Juan Antonio Cortés Ibáñez"
date: "January 21, 2018"
output:
  pdf_document:
    fig_width: 6
    fig_height: 4
    highlight: zenburn
    toc: true
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(ggplot2)
library(reshape)
library(tidyr)
library(discretization)
library(arules)
library(arulesViz)
```

## Estudio y visualización de los datos

El conjunto de datos elegido para hacer este trabajo ha sido `loan` el cual es un conjunto de datos artificial (es decir, son datos hipotéticos generados por ordenador) que acompaña al programa estadístico SPSS de IBM [¹]. Ya que el conjunto de datos se ofrece en diversos idiomas, hemos elegido aquél que tiene el nombre de las variables en español. Al haberlo convertido a un fichero CSV podemos abrirlo con el siguiente método:

[¹]: https://www.ibm.com/support/knowledgecenter/en/SSLVMB_20.0.0/com.ibm.spss.statistics.help/data_files.htm

```{r warning=FALSE}
loanData = read.csv("loan.csv", sep = ";", dec = ",")
```

Vemos que el conjunto de datos consta de 850 observaciones cada una de ellas con 12 variables. Cada observación se corresponde con un cliente del banco que ha solicitado un préstamo y del cual se almacenan datos tales como la edad, el nivel de educación o los ingresos, a parte de marcar si este cliente ha cometido impago o no.

Veamos ahora qué tipo de datos tenemos y si contienen valores perdidos:

```{r warning=FALSE}
data.types = sapply(loanData, class)
table(data.types)
sapply(loanData, function(x) sum(is.na(x)))
```

Vemos que tenemos seis (en realidad cuatro ya que dos de ellas, `impago` y `educ`, son categóricas y las trataremos a continuación) variables enteras y seis continuas. También vemos que la variable `impago` tiene 150 valores perdidos y esto es debido a que las primeras 700 observaciones del conjunto de datos son clientes de los cuales conocemos su morosidad, mientras que las últimas 150 (que están marcadas como valores perdidos) no tienen dicha información y se usarían en un problema de clasificación clásico. Procedemos a eliminar estas 150 observaciones que tienen valores perdidos y a poner los valores correctos (obtenidos de la web del departamente de matemáticas de la *Central Michigan University* [²]) a las dos variables categóricas que se han mencionado:

[²]:http://calcnet.mth.cmich.edu/org/spss/Prj_loan_data.htm

```{r warning=FALSE}
loanData = loanData[1:700,]
loanData$educ = factor(loanData$educ, labels = c("Did not complete high school", "High school degree", "Some college", "College degree","Post-undergraduate degree"))
loanData$impago = factor(loanData$impago, labels = c("No", "Yes"))
```

```{r eval=FALSE, include=FALSE}
Una vez que tenemos los datos correctamente, procedemos a crear los ítems negativos. Tal y como se menciona en el enunciado, los aplicaremos a una sola variable para evitar una explosión de combinaciones. La variable elegida es `impago` así que expandimos el `factor` para así desglosarlo en dos nuevas variables lógicas: `impago_true` e `impago_false`:
loanDataCopy <-  tidyr::spread(loanData, key = impago, value = impago)
colnames(loanDataCopy)[12] <- "impago_true"
colnames(loanDataCopy)[13] <- "impago_false"
loanDataCopy$impago_false[loanDataCopy$impago_false == FALSE] <- TRUE
loanDataCopy$impago_false[is.na(loanDataCopy$impago_false)] <- FALSE
loanDataCopy$impago_true[is.na(loanDataCopy$impago_true)] <- FALSE
```
# Reglas de asociación

El objetivo que planteamos es la obtención de reglas de asociación que nos permitan predecir si un cliente será moroso o no en función de las variables dadas en nuestro conjunto de datos.

## Discretización

Para poder aplicar las reglas de asociación necesitamos discretizar nuestros datos. En un entorno real habría que ayudarse de la opinión de un experto para elegir correctamente los intervalos a usar, ya que una buena elección de éstos impacta directamente en los resultados finales. Para esta ocasión, nos ayudaremos del paquete `discretization` el cual realiza una separación automática de los valores y nos libera de ese trabajo:

```{r warning=FALSE}
subset.numeric.dt = function(df) {
  return(df[,sapply(df, class) != "factor", drop = F])
}
subset.numeric = function(df) {
  return(cbind(subset.numeric.dt(df), impago = df$impago))
}
loanData = cbind(subset(loanData, select = -impago), loanData[,"impago", drop = F])
loanDataNumeric = subset.numeric(loanData)
loanDataRmNa = loanDataNumeric[1:700,]
cutp = disc.Topdown(loanDataRmNa)$cutp
loanDataDisc = loanDataRmNa
for (i in 1:(ncol(loanDataRmNa)-1)) {
  loanDataDisc[,i] = cut(loanDataRmNa[,i], cutp[[i]], include.lowest = T)
}
loanDataDisc = cbind(loanData[1:700,"educ",drop=F], loanDataDisc)
```

## Transacciones

Tras discretizar nuestros datos ya podemos obtener las transacciones. Usaremos el paquete `arules` para realizar todo lo relacionado con reglas de asociación:

```{r warning=FALSE}
loanTrans = as(loanDataDisc, "transactions")
```

Una vez obtenidas podemos ver un resumen de estas transacciones:

```{r warning=FALSE}
summary(loanTrans)
```
Vemos que tenemos 700 transacciones siendo las cinco más frecuentes las siguientes: `ingresos=(15.5,446]`, `deudaotro=[0.0456,12.8]`, `deudacred=[0.0117,5.56]`, `morapred1=[0.00052,0.571]` y `deudaingr=[0.4,15.5]`.


## Ítems e itemsets frecuentes

Antes de generar reglas podemos chequear de manera gráfica qué ítems son los más frecuentes (aunque la ejecución del apartado anterior ya nos los señalaba):

```{r warning=FALSE}
itemFrequencyPlot(loanTrans, support = 0.1, cex.names = 0.8)
```

Vemos que tenemos tres ítems con un soporte tremendamento alto (cercano al 100%): ` ingresos=(15.5,446]`, `deudaotro=[0.0456,12.8]` y `deudacred=(0.0117,5.56]`. Lo más probable es que estos ítems no nos sean de demasiada ayuda ya que aparecen en casi todos los casos y no nos ayudarían a obtener reglas de calidad.

Procedemos ahora a buscar los *itemsets* más frecuentes haciendo uso del método `apriori` que implementa el paquete `arules`. 
```{r warning=FALSE, out.width="400px", out.height="300px"}
iSLoan = apriori(loanTrans, parameter = list(support = 0.1, target="frequent"))
iSData = data.frame(itemset.size = as.factor(size(iSLoan)))
ggplot(iSData) + geom_bar(aes(itemset.size)) + ggtitle("Histograma de los itemsets")
```

Vemos que sigue una distribución normal siendo los *itemsets* de tamaño seis y siete los que más frecuencia tienen.

## Itemsets maximales y cerrados

A la vista del gráfico anterior, comprobamos que los *itemsets* con 5 y 6 elementos son muy frecuentes. A continuación se muestra el  histograma de los *itemsets* maximales con mayor soporte para ver qué forma (distribución) tiene:

```{r warning=FALSE, out.width="400px", out.height="300px"}
iSMLoan = iSLoan[is.maximal(iSLoan)]
inspect(head(sort(iSMLoan, by = "support"), 1))
iSMData = data.frame(itemset.size = as.factor(size(iSMLoan)))
ggplot(iSMData) + geom_bar(aes(itemset.size)) + ggtitle("Histograma de los itemsets maximales")
```

Ordenando los *itemsets* maximales por su soporte vemos que ninguno llega a un 50% y su distribución está sesgada a la derecha, es decir, los maximales más frecuentes son los de mayor tamaño.

Veamos ahora qué ocurre con los cerrados:

```{r warning=FALSE, out.width="400px", out.height="300px"}
iSCLoan = iSLoan[is.closed(iSLoan)]
inspect(head(sort(iSCLoan, by = "support"), 5))
length(iSCLoan)
iSCData = data.frame(itemset.size = as.factor(size(iSCLoan)))
ggplot(iSCData) + geom_bar(aes(itemset.size)) + ggtitle("Histograma de los itemsets cerrados")
```

Vemos que tenemos *itemsets* cerrados con un soporte altísimo y con una distribución mucho más cercana a la normal aunque ligeramente sesgada hacia los *itemsets* de menor tamaño. 

A continuación se muestra un conteo del total los *itemsets* de cada tipo, simplemente para tener una perspectiva general de cómo se reducen drásticamente conforme pasamos de "normales" a maximales y cerrados:

```{r warning=FALSE, out.width="400px", out.height="300px"}
dataFCM = data.frame(Frecuentes = length(iSLoan), Cerrados = length(iSCLoan), Maximales = length(iSMLoan))
dataFCM = melt(dataFCM)
ggplot(dataFCM) + geom_col(aes(variable, value, fill = variable)) + ggtitle("Histograma conjunto de los itemsets") + labs(x = "Itemsets", y = "Frecuencia")
```

## Obtención de las reglas de asociación
A la hora de obtener las reglas de asociación seguiremos el algoritmo `apriori` ya que el conjunto de datos es muy pequeño y no tendremos ningún tipo de problema con el rendimiento del proceso. Comenzaremos con el clásico soporte del 10% y confianza del 80%.
```{r include=FALSE}
aRules = apriori(loanTrans, parameter = list(support = 0.1, confidence = 0.8, minlen = 2))
```

```{r message=FALSE, warning=FALSE}
cat("Reglas obtenidas: ",summary(aRules)@length)
```

Viendo el resumen vemos que tenemos un total de 35410 reglas. Procedamos a ordenarlas por su confianza:

```{r warning=FALSE}
aRulesConf = sort(aRules, by = "confidence")
head(inspect(head(aRulesConf)), n=5)
head(quality(head(aRulesConf)), n=5)
```

Vemos que el resumen también nos devuelve el `lift`, una medida un poco más robusta que la confianza, así que podemos ordenar por ella y ver qué obtenemos:

```{r warning=FALSE}
aRulesLift = sort(aRules, by = "lift")
head(inspect(head(aRulesLift)), n=5)
head(quality(head(aRulesLift)), n=5)
```
Al igual que al ordenar por la confianza, vemos que tenemos bastantes reglas con un *lift* muy alto aunque con un soporte bastante bajo (cosa que no quiere decir que sean malas reglas, quizás esas justamente expliquen los casos específicos que vamos buscando).

## Estudio de las reglas deseadas
Vamos a observar las reglas cuyo *lift* sea superior a 2 o contengan información sobre el impago en el consecuente. Así podremos explorar reglas que nos lleven a predecir el impago, así como otras reglas de posible interés. Obtenemos directamente las reglas que impliquen impago así como su negada:

```{r warning=FALSE}
aRulesSelect = subset(aRules, subset = rhs %in% c("impago=Yes", "impago=No") | lift > 2)
```

Podemos eliminar las reglas redundantes haciendo lo siguiente:

```{r warning=FALSE}
aRulesSelect <- aRulesSelect[!is.redundant(aRulesSelect)]
subsetMatrix = is.subset(aRulesSelect, aRulesSelect)
subsetMatrix[lower.tri(subsetMatrix, diag = T)] <- FALSE
redundant = colSums(subsetMatrix, na.rm = T) >= 1
aRulesPruned = aRulesSelect[!redundant]
cat("Reglas no redundantes: ", summary(aRulesPruned)@length)
```

Con este filtrado reducimos el número de reglas a 15, algo mucho más manejable y fácil de estudiar.

## Otras medidas de interés

Llegados a este punto, podemos decir que hemos encontrado reglas que parecen ser interesantes, ya que nos permiten predecir la morosidad/no morosidad a partir de otros parámetros. Sin embargo, para confirmar que estas reglas son útiles deberemos estudiar otro tipo de medidas y cerciorarnos. En esta ocasión y tras leer las diferentes medidas que implementa el paquete [³] se han elegido la *hyperConfidence*, *leverage*, *phi* y la medida de entropía de *gini* (esta útlima por ser una medida clásica en muchos algoritmos).

[³]:https://www.rdocumentation.org/packages/arules/versions/1.5-5/topics/interestMeasure

```{r warning=FALSE}
myInterestMeasures = interestMeasure(aRulesPruned, measure = c("hyperConfidence", "leverage", "phi", "gini"), transactions = loanTrans)
quality(aRulesPruned) = cbind(quality(aRulesPruned), myInterestMeasures)
head(inspect(sort(aRulesPruned, by = "gini")),n=5)
```

## Visualización

Ahora podemos visualizar las reglas obtenidas con el paquete `arulesViz`. Existen varios tipos de visualización las cuales nos dan distintas perspectivas sobre las reglas encontradas. Se ha elegido la representación estilo *graph* por ser bastante clara al tratarse de pocas reglas:

```{r echo=FALSE, results='hide', fig.keep='all'}
plot(aRulesPruned, method="graph", control=list(type="items"))
```

Bien, ya que tenemos las reglas más interesantes ordenadas por diferentes medidas y graficadas podemos extraer conclusiones. 

Vemos que tenemos una única regla interesante con `impago=Yes` en el consecuente. Esta regla tiene un soporte bajo (también debido al desbalanceo que existe en los datos donde sólo el 20% de los ejemplos son de impagos confirmados) pero tiene una confianza y *lift* bastante altos, por lo que es interesante para predecir morosos en base al parámetros `morapred1`.

Por otro lado, vemos que las reglas con el consecuente negativo, es decir, `impago=No` son mucho más interesante de estudiar y también nos servirían para nuestro cometido de detectar la posible morosidad. Gracias a que tenemos muchos más datos de personas que no han cometido impago podemos obtener reglas más variadas y mejores. 

Por ejemplo, gracias a estas reglas podemos ver que el tipo de empleo está directamente relacionado con la morosidad y que los atributos de predicción que incluyen los datos (`morapred`) también son un buen estimador.

En definitiva, podemos predecir con cierto grado de garantías la morosidad de los clientes en base a las reglas obtenidas.
© 2019 GitHub, Inc.
Terms
Privacy
Security
Status
Help
Contact GitHub
Pricing
API
Training
Blog
About
Press h to open a hovercard with more details.